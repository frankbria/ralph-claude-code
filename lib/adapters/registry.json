{
    "version": "1.0.0",
    "default_adapter": "claude",
    "adapters": {
        "claude": {
            "file": "claude.sh",
            "display_name": "Claude Code",
            "description": "Anthropic's Claude Code CLI - AI-powered coding assistant",
            "install_command": "npm install -g @anthropic-ai/claude-code",
            "documentation": "https://docs.anthropic.com/en/docs/claude-code",
            "requirements": ["Node.js 18+", "Anthropic API key or Claude Code auth"],
            "features": ["streaming", "tools", "vision", "200k context"],
            "supported_models": ["claude-sonnet-4-20250514", "claude-3-5-sonnet", "claude-3-opus"],
            "default": true
        },
        "ocode": {
            "file": "ocode.sh",
            "display_name": "O-Code",
            "description": "The open source AI coding agent - multi-provider, multi-model support",
            "install_command": "bun add -g opencode-ai",
            "documentation": "https://github.com/pt-act/o-code",
            "requirements": ["Bun or Node.js 18+", "API key for chosen provider"],
            "features": ["streaming", "tools", "multi-model", "multi-provider", "session-management", "agents", "code-editing", "file-access"],
            "supported_models": ["anthropic/claude-*", "openai/gpt-*", "openai/o1", "google/gemini-*", "ollama/*"],
            "available_agents": ["build", "plan"],
            "default": false
        },
        "aider": {
            "file": "aider.sh",
            "display_name": "Aider",
            "description": "AI pair programming in your terminal",
            "install_command": "pip install aider-chat",
            "documentation": "https://aider.chat",
            "requirements": ["Python 3.8+", "API key for chosen model"],
            "features": ["multi-model", "git-integration", "code-editing", "local-models"],
            "supported_models": ["gpt-4-turbo", "gpt-4o", "claude-3-5-sonnet", "ollama/*"],
            "default": false
        },
        "ollama": {
            "file": "ollama.sh",
            "display_name": "Ollama",
            "description": "Run large language models locally - fully offline capable",
            "install_command": "curl -fsSL https://ollama.ai/install.sh | sh",
            "documentation": "https://ollama.ai",
            "requirements": ["Ollama installed", "At least one model pulled"],
            "features": ["local", "offline", "no-rate-limit", "multi-model"],
            "supported_models": ["codellama", "deepseek-coder", "llama3", "mistral", "phi3"],
            "default": false
        }
    },
    "feature_descriptions": {
        "streaming": "Real-time token streaming output",
        "tools": "Tool/function calling support",
        "vision": "Image input and analysis",
        "multi-model": "Support for multiple AI models",
        "multi-provider": "Support for multiple AI providers (OpenAI, Anthropic, Google, etc.)",
        "git-integration": "Automatic git commits and history",
        "code-editing": "Direct file editing capabilities",
        "local-models": "Support for locally running models",
        "local": "Runs entirely on local hardware",
        "offline": "Works without internet connection",
        "no-rate-limit": "No API rate limits",
        "session-management": "Persistent session support for multi-turn conversations",
        "agents": "Built-in agent modes (build, plan, etc.)",
        "file-access": "Full file system read/write access"
    }
}